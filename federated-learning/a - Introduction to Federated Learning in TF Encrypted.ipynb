{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction to Federated Learning in TF Encrypted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the powerful distributing computing platform provided by TensorFlow with the encrypted computations provided by TF Encrypted allows us to create a flexible federated learning system. Here we also leverage TF 2.0 which gives us even more power to debug, train and prepare models for deployment.\n",
    "\n",
    "While this is only a simulation and shouldn't be used in production this tutorial will help give you a foundation to continue to explore with federated learning and start thinking about how this could be further enhanced and deployed into a machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/Users/justinpatriquin/projects/tf-encrypted/tf_encrypted/operations/secure_random/secure_random_module_tf_2.0.0.so'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tf_encrypted as tfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we must define who our players are. This is only for local computations to help easily explore the power of federated learning. In a further part we'll show how this can be extended to work on actual servers running in the cloud.\n",
    "\n",
    "For the encrypted computations we need three players (server1, server2 and the crypto-producer). These servers work together to securely compute the mean of the gradients as seen later on.\n",
    "\n",
    "To host the model and data we need at least two players but to make it more interesting we're using four players. One for the model owner (responible for initial weights, updating the weights and evaluating the model) and three data owners who hold onto their own data and locally calculate the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = [\n",
    "    'server0', \n",
    "    'server1', \n",
    "    'crypto-producer', \n",
    "    'model-owner',\n",
    "    'data-owner-0',\n",
    "    'data-owner-1',\n",
    "    'data-owner-2',\n",
    "]\n",
    "config = tfe.EagerLocalConfig(players)\n",
    "\n",
    "tfe.set_config(config)\n",
    "tfe.set_protocol(tfe.protocol.Pond())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import some local helper functions. \n",
    "\n",
    "`BaseModelOwner` and `BaseDataOwner` will help us run the computations on the correct players and synchronize the communication so that each player is calculating the gradients for the same round. These base classes also allow us to easily extend these computations so that we can customize the computations are run.\n",
    "\n",
    "The other three functions contain some already implemented functions that we can use to start customizing our federated learning computations.\n",
    "\n",
    "- `default_model_fn` contains the code that does the forward and backward pass on the model\n",
    "- `secure_mean` uses encrypted computations to calculate the mean over the gradients\n",
    "- `evaluate_classifier` evaluates the model and returns the loss\n",
    "\n",
    "`split_dataset` is a helper function to split the dataset into chunks so that each data owner gets a unique set of the dataset. Its important to note this is only for simulation and doesn't make any sense in the real world.\n",
    "\n",
    "`download_mnist` is a helper function to save an mnist dataset to disk in the form of tfrecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from players import BaseModelOwner, BaseDataOwner\n",
    "from func_lib import default_model_fn, secure_mean, evaluate_classifier\n",
    "from util import split_dataset\n",
    "from download import download_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some globals that help customize the training loop. Note: `NUM_DATA_OWNERS` must match how many data owners are in the above configuration and `DATA_ITEMS` must match how many rows are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DATA_OWNERS = 3\n",
    "BATCH_SIZE = 256\n",
    "DATA_ITEMS = 60000\n",
    "BATCHES = DATA_ITEMS // NUM_DATA_OWNERS // BATCH_SIZE\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is subclassing the `BaseModelOwner` and `BaseDataOwner` classes.\n",
    "\n",
    "For the `ModelOwner` you can see we override three functions defined as abstract class methods in `BaseModelOwner`. Inside each of these functions we can customize the function that actually needs to be called. Here you can see us place in the functions that we've imported above (`default_model_fn`, `secure_mean` and `evaluate_classifier`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOwner(BaseModelOwner):\n",
    "  @classmethod\n",
    "  def model_fn(cls, data_owner):\n",
    "    return default_model_fn(data_owner)\n",
    "\n",
    "  @classmethod\n",
    "  def aggregator_fn(cls, model_gradients, model):\n",
    "    return secure_mean(model_gradients)\n",
    "\n",
    "  @classmethod\n",
    "  def evaluator_fn(cls, model_owner):\n",
    "    return evaluate_classifier(model_owner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for `BaseDataOwner` there is no customization needed for right so we leave it empty for now. We'll see how can add some customized function later in the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataOwner(BaseDataOwner):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the main parts of the computation we first need to download the MNIST dataset and split it amongst the parties. The data is downloaded to the directory `./data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Splitting dataset for 3 data owners. This is for simulation use only\n"
     ]
    }
   ],
   "source": [
    "download_mnist()\n",
    "split_dataset(\"./data\", NUM_DATA_OWNERS, DATA_ITEMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section is pretty normal TensorFlow where we define a simple `Sequential` model in Keras that will be able to classify the MNIST dataset. We set the loss and optimizer to variables so that we can pass them into the `ModelOwner` and the `DataOwner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential((\n",
    "    tf.keras.layers.Dense(512, input_shape=[None, 28 * 28],\n",
    "                          activation='relu'),\n",
    "    tf.keras.layers.Dense(10),\n",
    "))\n",
    "\n",
    "model.build()\n",
    "\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "opt = tf.keras.optimizers.Adam(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to initialize the model owner and the data owners. Pretty straight-forward but important to note that the names pass into the first arguments must match the names references in the above configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_owner = ModelOwner(\"model-owner\",\n",
    "                         \"{}/train.tfrecord\".format(\"./data\"),\n",
    "                         model, loss,\n",
    "                         optimizer=opt)\n",
    "\n",
    "# Simplify this with a loop?\n",
    "data_owners = [DataOwner(\"data-owner-{}\".format(i),\n",
    "                         \"{}/train{}.tfrecord\".format(\"./data\", i),\n",
    "                         model, loss,\n",
    "                         optimizer=opt)\n",
    "              for i in range(NUM_DATA_OWNERS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.fit` is the main event here. It takes in a few basic arguments and then orchestrators the computation using the cusotmized `ModelOwner` and `DataOwner` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/78 [=========================>....] - ETA: 1s - Loss: 0.1766\n",
      "Done training!!\n"
     ]
    }
   ],
   "source": [
    "model_owner.fit(data_owners, rounds=BATCHES, evaluate_every=10)\n",
    "\n",
    "print(\"\\nDone training!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
