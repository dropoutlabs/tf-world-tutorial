{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction to Federated Learning in TF Encrypted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the powerful distributing computing platform provided by TensorFlow with the encrypted computations provided by TF Encrypted allows us to create a flexible federated learning system. Here we also leverage TF 2.0 which gives us even more power to debug, train and prepare models for deployment.\n",
    "\n",
    "While this is only a simulation and shouldn't be used in production this tutorial will help give you a foundation to continue to explore with federated learning and start thinking about how this could be further enhanced and deployed into a machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_encrypted as tfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we must define who our players are. This is only for local computations to help easily explore the power of federated learning. In a further part we'll show how this can be extended to work on actual servers running in the cloud.\n",
    "\n",
    "For the encrypted computations we need three players (server1, server2 and the crypto-producer). These servers work together to securely compute the mean of the gradients as seen later on.\n",
    "\n",
    "To host the model and data we need at least four players. One for the model owner (responible for initial weights, updating the weights and evaluating the model) and three data owners who hold onto their own data and locally calculate the gradients.\n",
    "\n",
    "This tutorial and the following parts are intended to be run on servers rather than low-powered devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = [\n",
    "    'server0', \n",
    "    'server1', \n",
    "    'crypto-producer', \n",
    "    'model-owner',\n",
    "    'data-owner-0',\n",
    "    'data-owner-1',\n",
    "    'data-owner-2',\n",
    "]\n",
    "config = tfe.EagerLocalConfig(players)\n",
    "\n",
    "tfe.set_config(config)\n",
    "tfe.set_protocol(tfe.protocol.Pond())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import some local helper functions. \n",
    "\n",
    "`BaseModelOwner` and `BaseDataOwner` will help us run the computations on the correct players and synchronize the communication so that each player is calculating the gradients for the same round. These base classes also allow us to easily extend these computations so that we can customize the computations that are run.\n",
    "\n",
    "The other three functions contain some already implemented functions that we can use to start customizing our federated learning computations.\n",
    "\n",
    "- `default_model_fn` contains the code that does the forward and backward pass on the model\n",
    "- `secure_mean` uses encrypted computations to calculate the mean over the gradients\n",
    "- `evaluate_classifier` evaluates the model and returns the loss\n",
    "\n",
    "`split_dataset` is a helper function to split the dataset into chunks so that each data owner gets a unique subset of the dataset. Its important to note this is only for simulation and doesn't make any sense in the real world.\n",
    "\n",
    "`download_mnist` is a helper function to save an MNIST dataset to disk in the form of tfrecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from players import BaseModelOwner, BaseDataOwner\n",
    "from util import split_dataset\n",
    "from download import download_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some globals that help customize the training loop. Note: `NUM_DATA_OWNERS` must match how many data owners are in the above configuration and `DATA_ITEMS` must match how many rows are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DATA_OWNERS = 3\n",
    "BATCH_SIZE = 256\n",
    "DATA_ITEMS = 60000\n",
    "BATCHES = DATA_ITEMS // NUM_DATA_OWNERS // BATCH_SIZE\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the main functions we'll use to define the model training process.  This includes functions we'll need to interfact with the `BaseModelOwner` below -- a `model_fn` and an `aggregation_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_model_fn(data_owner):\n",
    "  \"\"\"Runs a single training step!\"\"\"\n",
    "  x, y = next(data_owner.dataset)\n",
    "\n",
    "  with tf.name_scope(\"gradient_computation\"):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      preds = data_owner.model(x)\n",
    "      loss = tf.reduce_mean(data_owner.loss(y, preds, from_logits=True))\n",
    "\n",
    "    grads = tape.gradient(loss, data_owner.model.trainable_variables)\n",
    "\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secure_mean(collected_inputs):\n",
    "  \"\"\" securely calculates the mean of the collected_inputs \"\"\"\n",
    "\n",
    "  with tf.name_scope('secure_mean'):\n",
    "\n",
    "    aggr_inputs = [\n",
    "        tfe.add_n(inputs) / len(inputs)\n",
    "        for inputs in collected_inputs\n",
    "    ]\n",
    "\n",
    "    # Reveal aggregated values & cast to native tf.float32\n",
    "    aggr_inputs = [tf.cast(inp.reveal().to_native(), tf.float32)\n",
    "                   for inp in aggr_inputs]\n",
    "\n",
    "    return aggr_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, here's a plaintext version of that secure mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(collected_inputs):\n",
    "  \"\"\" securely calculates the mean of the collected_inputs \"\"\"\n",
    "\n",
    "  with tf.name_scope('mean'):\n",
    "\n",
    "    aggr_inputs = [\n",
    "        tf.add_n(inputs) / len(inputs)\n",
    "        for inputs in collected_inputs\n",
    "    ]\n",
    "\n",
    "    return aggr_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we'll define how the ModelOwner expects to validate the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model_owner):\n",
    "  \"\"\"Runs a validation step!\"\"\"\n",
    "  x, y = next(model_owner.evaluation_dataset)\n",
    "\n",
    "  with tf.name_scope('validate'):\n",
    "    predictions = model_owner.model(x)\n",
    "    loss = tf.reduce_mean(model_owner.loss(y, predictions, from_logits=True))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is subclassing the `BaseModelOwner` and `BaseDataOwner` classes.\n",
    "\n",
    "For the `ModelOwner` you can see we override three functions defined as abstract class methods in `BaseModelOwner`. Inside each of these functions we can customize the function that actually needs to be called. Here you can see us place in the functions that we've defined above (`default_model_fn`, `secure_mean` and `evaluate_classifier`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOwner(BaseModelOwner):\n",
    "  @classmethod\n",
    "  def model_fn(cls, data_owner):\n",
    "    return default_model_fn(data_owner)\n",
    "\n",
    "  @classmethod\n",
    "  def aggregator_fn(cls, model_gradients, model):\n",
    "    return secure_mean(model_gradients)\n",
    "\n",
    "  @classmethod\n",
    "  def evaluator_fn(cls, model_owner):\n",
    "    return evaluate_classifier(model_owner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for `BaseDataOwner` there is no customization needed for right so we leave it empty for now. We'll see how can add some customized function later in the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataOwner(BaseDataOwner):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the main parts of the computation we first need to download the MNIST dataset and split it amongst the parties. The data is downloaded to the directory `./data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Splitting dataset for 3 data owners. This is for simulation use only\n"
     ]
    }
   ],
   "source": [
    "download_mnist()\n",
    "split_dataset(\"./data\", NUM_DATA_OWNERS, DATA_ITEMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section is pretty normal TensorFlow where we define a simple `Sequential` model in Keras that will be able to classify the MNIST dataset. We set the loss and optimizer to variables so that we can pass them into the `ModelOwner` and the `DataOwner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential((\n",
    "    tf.keras.layers.Dense(512, batch_input_shape=[BATCH_SIZE, 28 * 28],\n",
    "                          activation='relu'),\n",
    "    tf.keras.layers.Dense(10),\n",
    "))\n",
    "\n",
    "model.build()\n",
    "\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "opt = tf.keras.optimizers.Adam(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to initialize the model owner and the data owners. Pretty straight-forward but important to note that the names pass into the first arguments must match the names references in the above configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_owner = ModelOwner(\"model-owner\",\n",
    "                         \"{}/train.tfrecord\".format(\"./data\"),\n",
    "                         model, loss,\n",
    "                         optimizer=opt)\n",
    "\n",
    "# Simplify this with a loop?\n",
    "data_owners = [DataOwner(\"data-owner-{}\".format(i),\n",
    "                         \"{}/train{}.tfrecord\".format(\"./data\", i),\n",
    "                         model, loss,\n",
    "                         optimizer=opt)\n",
    "              for i in range(NUM_DATA_OWNERS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.fit` is the main event here. It takes in a few basic arguments and then orchestrators the computation using the customized `ModelOwner` and `DataOwner` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/78 [=========================>....] - ETA: 0s - Loss: 0.1252\n",
      "Done training!!\n"
     ]
    }
   ],
   "source": [
    "model_owner.fit(data_owners, rounds=BATCHES, evaluate_every=10)\n",
    "\n",
    "print(\"\\nDone training!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jasonmancuso/tf-world/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../saved_fl_model/assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model_owner.model, \"../saved_fl_model\", save_format=\"tf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
