{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Intro to Private Training with Remote Execution\n",
    "\n",
    "**Note that this tutorial was originally designed part of [OpenMined PySyft-TensorFlow tutorials](https://github.com/OpenMined/PySyft-TensorFlow/tree/master/examples)**\n",
    "\n",
    "In the last section, we learned about PointerTensors, which create the underlying infrastructure we need for privacy preserving Deep Learning. In this section, we're going to see how to use these basic tools to train our first deep learning model using remote execution.\n",
    "\n",
    "### Why use remote execution?\n",
    "\n",
    "Let's say you are an AI startup who wants to build a deep learning model to detect [diabetic retinopathy (DR)](https://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html), which is the fastest growing cause of blindness. Before training your model, the first step would be to acquire a dataset of retinopathy images with signs of DR. One approach could be to work with a hospital and ask them to send you a copy of this dataset. However because of the sensitivity of the patients' data, the hospital might be exposed to liability risks.\n",
    "\n",
    "\n",
    "That's where remote execution comes into the picture. Instead of bringing training data to the model (a central server), you bring the model to the training data (wherever it may live). In this case, it would be the hospital.\n",
    "\n",
    "The idea is that this allows whoever is creating the data to own the only permanent copy, and thus maintain control over who ever has access to it. Pretty cool, eh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.1 - Private Training on Fashion MNIST\n",
    "\n",
    "For this tutorial, we will train a model on the [Fashion MNIST dataset](https://www.tensorflow.org/datasets/catalog/fashion_mnist) to classify images of clothing.\n",
    "\n",
    "We can assume that we have a remote worker named Bob who owns the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import syft as sy\n",
    "\n",
    "hook = sy.TensorFlowHook(tf)\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the Fashion MNIST data from `tensorflow_datasets`. In the cell below, we are converting the data from [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to `tf.Tensor` in order to have the PySyft functionalities. \n",
    "\n",
    "Note that adding tf.data.Dataset support is on the roadmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1023 14:06:02.998682 4731491776 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset = tfds.load(name=\"fashion_mnist\")\n",
    "\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "# we are generating this large batch to convert\n",
    "# the tf.data.Dataset into tf.Tensor\n",
    "train_dataset = next(iter(train_dataset.batch(60000)))\n",
    "test_dataset = next(iter(test_dataset.batch(10000)))\n",
    "\n",
    "x_train, y_train = train_dataset['image'], train_dataset['label']\n",
    "x_test, y_test = test_dataset['image'], test_dataset['label']\n",
    "\n",
    "x_train = tf.cast(x_train, tf.float32) / 255.0\n",
    "x_test = tf.cast(x_test, tf.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As decribed in Part 1, we can send this data to Bob with the `send` method on the `tf.Tensor`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ptr = x_train.send(bob)\n",
    "y_train_ptr = y_train.send(bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! We have everything to start experimenting. To train our model on Bob's machine, we just have to perform the following steps:\n",
    "\n",
    "- Define a model, including optimizer and loss\n",
    "- Send the model to Bob\n",
    "- Start the training process\n",
    "- Get the trained model back\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(28, 28 ,1)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(62, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with optimizer, loss and metrics\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have defined your model, you can simply send it to Bob calling the `send` method. It's the exact same process as sending a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1023 14:06:12.109671 4731491776 deprecation.py:506] From /anaconda3/envs/tf20/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model_ptr = model.send(bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[ObjectPointer | me:35042159074 -> bob:70988624973]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a pointer pointing to the model on Bob's machine. We can validate that's the case by inspecting the attribute `_objects` on the virtual worker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.saving.saved_model.load.Sequential object at 0x155d466a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob._objects[model_ptr.id_at_location]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready to start training our model on this remote dataset. You can call `fit` and pass `x_train_ptr` `y_train_ptr` which are pointing to Bob's data. Note that's the exact same interface as normal `tf.keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 13s 262us/sample - loss: 0.4949 - accuracy: 0.8225 - val_loss: 0.3591 - val_accuracy: 0.8712\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 12s 244us/sample - loss: 0.3124 - accuracy: 0.8876 - val_loss: 0.3189 - val_accuracy: 0.8856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15646db00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ptr.fit(x_train_ptr, y_train_ptr, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! you have trained your model acheiving an accuracy greater than 95%.\n",
    "\n",
    "You can get your trained model back by just calling `get` on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.saving.saved_model.load.Sequential object at 0x1582ebcf8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gotten = model_ptr.get()\n",
    "\n",
    "model_gotten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to see if your model can generalize by assessing its accuracy on an holdout dataset. You can simply call `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/1 - 1s - loss: 0.2104 - accuracy: 0.8783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33301980676651, 0.8783]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gotten.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! The model remotely trained on Bob's data is more than 87% accurate on this holdout dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model doesn't fit into the Sequential paradigm, you can use Keras's functional API, or even subclass [tf.keras.Model](https://www.tensorflow.org/guide/keras/custom_layers_and_models#building_models) to create custom models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 10s 216us/sample - loss: 0.4553 - accuracy: 0.8369 - val_loss: 0.3600 - val_accuracy: 0.8686\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 10s 206us/sample - loss: 0.2949 - accuracy: 0.8926 - val_loss: 0.2886 - val_accuracy: 0.8958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15aafac50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomModel, self).__init__(name='custom_model')\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv_1 = Conv2D(16, 3, padding='same', activation='relu', input_shape=(28, 28 ,1))\n",
    "        self.maxpool_1 = MaxPooling2D()\n",
    "        self.conv_2 = Conv2D(32, 3, padding='same', activation='relu')\n",
    "        self.maxpool_2 = MaxPooling2D()\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(128, activation='relu')\n",
    "        self.dense_2 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.maxpool_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        return self.dense_2(x)\n",
    "              \n",
    "model = CustomModel(10)\n",
    "\n",
    "# need to call the model on dummy data before sending it\n",
    "# in order to set the input shape (required when saving to SavedModel)\n",
    "model.predict(tf.ones([1, 28, 28, 1]))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_ptr = model.send(bob)\n",
    "\n",
    "model_ptr.fit(x_train_ptr, y_train_ptr, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Done!\n",
    "\n",
    "And voilà! We have trained a Deep Learning model on Bob's data by sending the model to him. Never in this process do we ever see or request access to the underlying training data! We preserve the privacy of Bob!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join OpenMined Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
